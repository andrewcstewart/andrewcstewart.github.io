[
  {
    "path": "posts/2022-07-01-v010-of-dbt-uaparser-released/",
    "title": "v0.1.0 of dbt-uaparser released",
    "description": "User-Agent strings encode information about users that visit sites.  This `dbt` package implements a User Defined Function that leverages UA parsing code that can be used within database queries.",
    "author": [
      {
        "name": "Andrew Stewart",
        "url": {}
      }
    ],
    "date": "2022-07-01",
    "categories": [
      "dbt",
      "data engineering",
      "packages"
    ],
    "contents": "\nUser-Agent strings encode information about users that visit sites,\nincluding things like OS, device model, version numbers, etc.\nSome examples:\nMozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11\nMozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.7 (KHTML, like Gecko) RockMelt/0.8.36.78 Chrome/7.0.517.44 Safari/534.7\nMozilla/5.0 (Macintosh; U; PPC Mac OS X 10_4_11; ja-jp) AppleWebKit/533.16 (KHTML, like Gecko) Version/4.1 Safari/533.16\nMozilla/5.0 (Macintosh; U; PPC Mac OS X 10_4_11; de-de) AppleWebKit/533.16 (KHTML, like Gecko) Version/4.1 Safari/533.16\nMozilla/5.0 (Macintosh; U; Intel Mac OS X 10_7; en-us) AppleWebKit/533.4 (KHTML, like Gecko) Version/4.1 Safari/533.4\nMozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_2; nb-no) AppleWebKit/533.16 (KHTML, like Gecko) Version/4.1 Safari/533.1\nParsing these strings from log data can often get complicated. When\nprocessing this kind of data within a data warehouse context, you likely\nfind yourself trying to cobble together a nested mess of string parsing\nfunctions and conditional logic to account for any possible string you\nmay encounter. This is a great use case for User Defined\nFunctions (UDFs), which typically allow one to define custom\nroutines that can be used like native instructions in-database.\nDepending on the database engine, these may just be essentially SQL\nmacros, or they may allow you to leverage other languages like\nJavascript or Python.\nEven with UDFs, the amount of logic needed to define robust UA\nparsing functions can be intimidating to manage. Fortunately there are\nalready many comprehensive parsing libraries out there that account for\nall the different formats and structures one may run into when parsing\nUA strings. Using Snowflake\nas an example, we can define a UDF that interfaces with a UA\nparser:\n\n\n\nOnce the UDF has been defined, we can use it within our queries like\nthis:\n\n\n\nUsing dbt we can define a macro\nthat will create this UDF in our data warehouse. I authored a dbt packge\nthat does exactly that: https://github.com/andrewcstewart/dbt-uaparser\nThis dbt package implements a User Defined Function that\nleverages the UA parsing code from https://faisalman.github.io/ua-parser-js/. You can find\nthe documentation here.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-01T19:20:09+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-11-v010-of-meltano-kedro-plugin-released/",
    "title": "v0.1.0 of Meltano kedro plugin released",
    "description": "Integrating Kedro machine learning pipelines in your Meltano ELT projects.",
    "author": [
      {
        "name": "Andrew Stewart",
        "url": {}
      }
    ],
    "date": "2022-06-11",
    "categories": [
      "data engineering",
      "packages",
      "machine learning",
      "kedro"
    ],
    "contents": "\nDistill is a publication format for scientific and technical writing,\nnative to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-01T19:20:30+00:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Andrew Stewart's Technical Blog",
    "description": "Welcome to our new blog, Andrew Stewart's Technical Blog. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Andrew Stewart",
        "url": {}
      }
    ],
    "date": "2022-05-31",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2022-06-12T17:35:56+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-31-new-meltano-plugin-files-gitpod/",
    "title": "New Meltano plugin: files-gitpod",
    "description": "A plugin for adding a Gitpod configuration to a Meltano project.",
    "author": [
      {
        "name": "Andrew Stewart",
        "url": "https://andrewcstewart.github.io"
      }
    ],
    "date": "2022-05-29",
    "categories": [
      "data engineering",
      "packages"
    ],
    "contents": "\nGitpod (https://www.gitpod.io/) is a great tool for spinning up\nfresh, disposable, containerized development environments built with\nCI/CD automation. It’s a fantastic development environment for\nMeltano.com projects.\nGetting started with Gitpod generally involves adding a\n.gitpod.yml configuration file to your project, as well as\nany supporting files (Dockerfiles, docker-compose configuration,\netc).\nI put together a simple Meltano files plugin to add a\nbasic Gitpod setup to a Meltano project: https://github.com/andrewcstewart/files-gitpod\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-01T19:14:08+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-12-28-confusion-matrix-shinyapp/",
    "title": "Confusion Matrix ShinyApp",
    "description": "Generating a confusion matrix in Shiny.",
    "author": [
      {
        "name": "Andrew Stewart",
        "url": {}
      }
    ],
    "date": "2019-12-28",
    "categories": [
      "r",
      "shiny"
    ],
    "contents": "\n\nDistill is a publication format for scientific and technical writing,\nnative to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": "posts/2019-12-28-confusion-matrix-shinyapp/screenshot.png",
    "last_modified": "2022-07-01T19:17:19+00:00",
    "input_file": {},
    "preview_width": 2934,
    "preview_height": 1844
  },
  {
    "path": "posts/2018-11-18-awesome-rust-for-biology-algorithms/",
    "title": "Awesome Rust for Biology Algorithms",
    "description": "Rust is increasingly being used for performant implementations of various algorithms, including those used in computational biology",
    "author": [
      {
        "name": "Andrew Stewart",
        "url": {}
      }
    ],
    "date": "2018-11-18",
    "categories": [
      "rust",
      "algorithms",
      "bioinformatics"
    ],
    "contents": "\nRust is increasingly\nbeing used for performant implementations of various algorithms,\nincluding those used in computational biology. Some examples include a\nminhash algorithm implemented by OneCodex and a pseudo-aligner by 10X\nGenomics.\nI thought it would be useful to start compiling a collection of these\nrust implementations as they appear, and so I’ve started an awesome-rust-biology-algorithms\nlist to track them.\n\n\n\n",
    "preview": "https://raw.githubusercontent.com/sindresorhus/awesome/master/media/logo.svg?sanitize=true",
    "last_modified": "2022-07-01T19:30:56+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-05-20-simple-data-frame-readwrite-with-s3/",
    "title": "Simple data frame read/write with S3",
    "description": "Being able to conveniently exchange data sets via S3 in either R or Python can really aid reproducibility.",
    "author": [
      {
        "name": "Andrew Stewart",
        "url": {}
      }
    ],
    "date": "2018-05-20",
    "categories": [
      "r",
      "python"
    ],
    "contents": "\nEveryone is always asking the age old question: R or Python? Why not\nboth? I refuse to choose! They each have their own strengths and\nweaknesses, and I often find myself seamlessly interweaving between the\ntwo depending on the task at hand.\nBeing ambidextrous with R and Python has never been easier, as\ncross-pollination across the two languages has resulted in many of the\nsame constructs now existing for both. For example, data frame\nimplementations such as pandas and tibble are\nlargely converging on many of the same patterns and tooling. One of the\nmost important features for interoperability is being able to serialize\nand de-serialize data frames between the two - which both readily do\nnatively via CSV or similar formats. Both can also transmit data via\nmirrored connectors for all sorts of various databases. There are even\nprojects like feather which\nare specifically targeted towards inter-language serialization between R\nand Python.\nBut what about S3? Whenever I’m handling even moderately large data\nsets, it’s incredibly convenient to simply stash files in an S3 bucket.\nDoing so also helps advance the cause of reproducible research, since S3\nobjects have universally accessible, unique URLs and most folks are\nlikely to have AWS credentials already setup from the AWS CLI or\nsimilar.\nSo how can one easily use S3 as a conduit for exchanging data sets\nbetween sessions and even between languages like Python and R? It would\nbe fairly simple to just read/write a file that can be\ndownloaded/uploaded from S3, but it turns out there are some very\nconvenient libraries to help streamline the process.\nFor R, there’s the aws.s3 package, which is part of the\nlarger cloudyr project (basically boto for R).\nThere are all kinds of nice functions in this package, but one of my\nfavorite is s3read_using, which allows you to utilize your\nown file parser such as read_csv from\ntidyverse/readr. The example below illustrates\nhow to load the contents of an S3 object into a data.frame,\nall with one line of code.\nlibrary(tidyverse)\nlibrary(\"aws.s3\")\n\ndf <- aws.s3::s3read_using(read_csv, object = \"s3://acs-blog-data/iris.csv\")\nLikewise, we can write our data frames to S3 using\ns3write_using:\naws.s3::s3write_using(iris, write_csv, object = \"s3://acs-blog-data/iris.csv\")\nWe can do the same thing in Python by combining pandas with the handy\ns3fs package, which sits on top of boto3.\nimport s3fs\nimport pandas as pd\n\ns3 = s3fs.S3FileSystem(anon=False)\nwith s3.open(\"s3://acs-blog-data/iris.csv\") as f:\n    df = pd.read_csv(f)\nSimilarly, writing to S3 should be as simple as the following\nexample, although I’ve initially encountered an error.\nwith s3.open(\"s3://acs-blog-data/iris.csv\", 'wb') as f:\n    df.write_csv(f)\nBeing able to conveniently exchange data sets via S3 in either R or\nPython can really aid reproducibility. Jupyter Notebooks and RMarkdown\nfiles can be shared via conventional version control, while sharing of\npotentially large data sets can be done over S3.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-06-26T21:33:53+00:00",
    "input_file": {}
  }
]
